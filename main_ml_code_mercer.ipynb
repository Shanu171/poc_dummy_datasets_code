{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "300e14ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import joblib  # For saving the model\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fffd3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n",
      "--- Starting Model Training (Part 1) ---\n",
      "Successfully loaded mercer_project/claims_200k.csv and mercer_project/membership_120k.csv\n",
      "Data cleaning complete. Claims: 199845, Members: 120000\n",
      "Creating member-year scaffold...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 362\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training (Part 1) Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 362\u001b[0m     \u001b[43mmain_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 324\u001b[0m, in \u001b[0;36mmain_train\u001b[1;34m()\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load data. Exiting.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 324\u001b[0m df_master \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_scaffold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_claims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_members\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTART_YEAR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEND_YEAR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m df_model_data, _ \u001b[38;5;241m=\u001b[39m engineer_features(df_master\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df_model_data\u001b[38;5;241m.\u001b[39mempty:\n",
      "Cell \u001b[1;32mIn[19], line 123\u001b[0m, in \u001b[0;36mcreate_scaffold\u001b[1;34m(df_claims, df_members, start_year, end_year)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (series\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(keyword, case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, na\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\u001b[38;5;241m.\u001b[39many()\n\u001b[0;32m    111\u001b[0m aggregation_rules \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_claim_amount\u001b[39m\u001b[38;5;124m'\u001b[39m: (CLAIM_AMOUNT_COL, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_claim_count\u001b[39m\u001b[38;5;124m'\u001b[39m: (CLAIM_AMOUNT_COL, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhad_chemo\u001b[39m\u001b[38;5;124m'\u001b[39m: (CONDITION_CATEGORY_COL, \u001b[38;5;28;01mlambda\u001b[39;00m s: had_condition(s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChemo\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m    121\u001b[0m }\n\u001b[1;32m--> 123\u001b[0m df_agg \u001b[38;5;241m=\u001b[39m df_claims\u001b[38;5;241m.\u001b[39mgroupby([CLAIM_MEMBER_ID_COL, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclaim_year\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39magg(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maggregation_rules)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m    124\u001b[0m df_agg \u001b[38;5;241m=\u001b[39m df_agg\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclaim_year\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m, CLAIM_MEMBER_ID_COL: MEMBER_ID_COL})\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# --- B. Create the master (member, year) scaffold ---\u001b[39;00m\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:1432\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1429\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[0;32m   1431\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m-> 1432\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\apply.py:190\u001b[0m, in \u001b[0;36mApply.agg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\apply.py:423\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\apply.py:1608\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[1;34m(self, op_name)\u001b[0m\n\u001b[0;32m   1603\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[0;32m   1605\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[0;32m   1606\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1607\u001b[0m ):\n\u001b[1;32m-> 1608\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1611\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[0;32m   1612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\apply.py:496\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[1;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m         results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m key_data\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[1;32m--> 496\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), op_name)(how, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys, results\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\apply.py:497\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    493\u001b[0m         results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m key_data\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 497\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), op_name)(how, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys, results\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:257\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine\n\u001b[0;32m    256\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[1;32m--> 257\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_multiple_funcs(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m relabeling:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# columns is not narrowed by mypy from relabeling flag\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:362\u001b[0m, in \u001b[0;36mSeriesGroupBy._aggregate_multiple_funcs\u001b[1;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, func) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arg):\n\u001b[0;32m    361\u001b[0m         key \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mOutputKey(label\u001b[38;5;241m=\u001b[39mname, position\u001b[38;5;241m=\u001b[39midx)\n\u001b[1;32m--> 362\u001b[0m         results[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, DataFrame) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m concat\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:291\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_constructor(\n\u001b[0;32m    284\u001b[0m         [],\n\u001b[0;32m    285\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    286\u001b[0m         index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mresult_index,\n\u001b[0;32m    287\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m    288\u001b[0m     )\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mnkeys \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_agg_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_agg_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:327\u001b[0m, in \u001b[0;36mSeriesGroupBy._python_agg_general\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: func(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    326\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\n\u001b[1;32m--> 327\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m res \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_constructor(result, name\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_aggregated_output(res)\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:864\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[1;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;66;03m# we can preserve a little bit more aggressively with EA dtype\u001b[39;00m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m#  because maybe_cast_pointwise_result will do a try/except\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m#  with _from_sequence.  NB we are assuming here that _from_sequence\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;66;03m#  is sufficiently strict that it casts appropriately.\u001b[39;00m\n\u001b[0;32m    862\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 864\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_series_pure_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preserve_dtype:\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:885\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[1;34m(self, obj, func)\u001b[0m\n\u001b[0;32m    882\u001b[0m splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_splitter(obj, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[1;32m--> 885\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    886\u001b[0m     res \u001b[38;5;241m=\u001b[39m extract_result(res)\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m initialized:\n\u001b[0;32m    889\u001b[0m         \u001b[38;5;66;03m# We only do this validation on the first iteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:324\u001b[0m, in \u001b[0;36mSeriesGroupBy._python_agg_general.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    322\u001b[0m     alias \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39m_builtin_table_alias[func]\n\u001b[0;32m    323\u001b[0m     warn_alias_replacement(\u001b[38;5;28mself\u001b[39m, orig_func, alias)\n\u001b[1;32m--> 324\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: func(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    326\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\n\u001b[0;32m    327\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39magg_series(obj, f)\n",
      "Cell \u001b[1;32mIn[19], line 115\u001b[0m, in \u001b[0;36mcreate_scaffold.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhad_condition\u001b[39m(series, keyword):\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (series\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(keyword, case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, na\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\u001b[38;5;241m.\u001b[39many()\n\u001b[0;32m    111\u001b[0m aggregation_rules \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_claim_amount\u001b[39m\u001b[38;5;124m'\u001b[39m: (CLAIM_AMOUNT_COL, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_claim_count\u001b[39m\u001b[38;5;124m'\u001b[39m: (CLAIM_AMOUNT_COL, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_los\u001b[39m\u001b[38;5;124m'\u001b[39m: (CLAIM_LOS_COL, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_inpatient\u001b[39m\u001b[38;5;124m'\u001b[39m: (CLAIM_TYPE_COL, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mInpatient\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_conditions_count\u001b[39m\u001b[38;5;124m'\u001b[39m: (CONDITION_CATEGORY_COL, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnunique\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhad_cardio\u001b[39m\u001b[38;5;124m'\u001b[39m: (CONDITION_CATEGORY_COL, \u001b[38;5;28;01mlambda\u001b[39;00m s: had_condition(s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCardio\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhad_musculo\u001b[39m\u001b[38;5;124m'\u001b[39m: (CONDITION_CATEGORY_COL, \u001b[38;5;28;01mlambda\u001b[39;00m s: had_condition(s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMusculo\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhad_gastro\u001b[39m\u001b[38;5;124m'\u001b[39m: (CONDITION_CATEGORY_COL, \u001b[38;5;28;01mlambda\u001b[39;00m s: had_condition(s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGastro\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhad_chemo\u001b[39m\u001b[38;5;124m'\u001b[39m: (CONDITION_CATEGORY_COL, \u001b[38;5;28;01mlambda\u001b[39;00m s: had_condition(s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChemo\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m    121\u001b[0m }\n\u001b[0;32m    123\u001b[0m df_agg \u001b[38;5;241m=\u001b[39m df_claims\u001b[38;5;241m.\u001b[39mgroupby([CLAIM_MEMBER_ID_COL, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclaim_year\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39magg(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maggregation_rules)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m    124\u001b[0m df_agg \u001b[38;5;241m=\u001b[39m df_agg\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclaim_year\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m, CLAIM_MEMBER_ID_COL: MEMBER_ID_COL})\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\series.py:6471\u001b[0m, in \u001b[0;36mSeries.any\u001b[1;34m(self, axis, bool_only, skipna, **kwargs)\u001b[0m\n\u001b[0;32m   6469\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_logical_func((), kwargs, fname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6470\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 6471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanany\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43many\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6474\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbool_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilter_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbool\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6478\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\series.py:6457\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   6452\u001b[0m     \u001b[38;5;66;03m# GH#47500 - change to TypeError to match other methods\u001b[39;00m\n\u001b[0;32m   6453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   6454\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-numeric dtypes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6456\u001b[0m     )\n\u001b[1;32m-> 6457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(delegate, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\POC_Project_LTI\\venv\\lib\\site-packages\\pandas\\core\\nanops.py:520\u001b[0m, in \u001b[0;36mnanany\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;124;03mCheck if any elements along an axis evaluate to True.\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;124;03mFalse\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miub\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;66;03m# GH#26032 fastpath\u001b[39;00m\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;66;03m# error: Incompatible return value type (got \"Union[bool_, ndarray]\",\u001b[39;00m\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;66;03m# expected \"bool\")\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;66;03m# GH#34479\u001b[39;00m\n\u001b[0;32m    524\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with datetime64 dtypes is deprecated and will raise in a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuture version. Use (obj != pd.Timestamp(0)).any() instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    528\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    529\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import joblib  # For saving the model\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION: Based on your dummy data ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- File Paths ---\n",
    "CLAIM_FILE_PATH = 'mercer_project/claims_200k.csv'\n",
    "MEMBER_FILE_PATH = 'mercer_project/membership_120k.csv'\n",
    "\n",
    "# --- Claim Table Column Names ---\n",
    "CLAIM_AMOUNT_COL = 'Claim Amount'\n",
    "CLAIM_DATE_COL = 'Incurred Date'\n",
    "CLAIM_MEMBER_ID_COL = 'Unique Member Reference'\n",
    "CLAIM_TYPE_COL = 'Claim Type'\n",
    "CLAIM_LOS_COL = 'Calculated Length of Service'\n",
    "CONDITION_CATEGORY_COL = 'Condition Category'\n",
    "\n",
    "# --- Membership Table Column Names ---\n",
    "MEMBER_ID_COL = 'Unique Member Reference'\n",
    "MEMBER_DOB_COL = 'Year of Birth'\n",
    "MEMBER_JOIN_DATE_COL = 'Original Date of Joining'\n",
    "MEMBER_GENDER_COL = 'Gender'\n",
    "MEMBER_POSTCODE_COL = 'Short Post Code of Member'\n",
    "MEMBER_SCHEME_COL = 'Scheme Category/ Section Name'\n",
    "MEMBER_STATUS_COL = 'Status of Member'\n",
    "\n",
    "# --- Modeling Parameters ---\n",
    "START_YEAR = 2019\n",
    "END_YEAR = 2025  # The last year of *known* data\n",
    "VALIDATION_YEAR = 2025  # We will predict 2025 to test the model\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. LOAD AND PREPARE DATA ---\n",
    "# ==============================================================================\n",
    "\n",
    "def load_data(claim_path, member_path):\n",
    "    \"\"\"Loads and performs initial cleaning of claim and member files.\"\"\"\n",
    "    try:\n",
    "        try:\n",
    "            df_claims = pd.read_csv(claim_path)\n",
    "            df_members = pd.read_csv(member_path)\n",
    "        except UnicodeDecodeError:\n",
    "            df_claims = pd.read_csv(claim_path, encoding='latin1')\n",
    "            df_members = pd.read_csv(member_path, encoding='latin1')\n",
    "            \n",
    "        print(f\"Successfully loaded {claim_path} and {member_path}\")\n",
    "\n",
    "        # --- Clean Claims Data ---\n",
    "        claim_cols_to_keep = [\n",
    "            CLAIM_MEMBER_ID_COL, CLAIM_DATE_COL, CLAIM_AMOUNT_COL,\n",
    "            CLAIM_TYPE_COL, CLAIM_LOS_COL, CONDITION_CATEGORY_COL\n",
    "        ]\n",
    "        df_claims = df_claims[claim_cols_to_keep]\n",
    "        \n",
    "        df_claims[CLAIM_DATE_COL] = pd.to_datetime(df_claims[CLAIM_DATE_COL], errors='coerce')\n",
    "        df_claims['claim_year'] = df_claims[CLAIM_DATE_COL].dt.year\n",
    "        df_claims[CLAIM_AMOUNT_COL] = pd.to_numeric(df_claims[CLAIM_AMOUNT_COL], errors='coerce')\n",
    "        df_claims[CLAIM_LOS_COL] = pd.to_numeric(df_claims[CLAIM_LOS_COL], errors='coerce')\n",
    "        df_claims = df_claims.dropna(subset=[CLAIM_DATE_COL, CLAIM_MEMBER_ID_COL, CLAIM_AMOUNT_COL])\n",
    "        df_claims = df_claims[df_claims['claim_year'].between(START_YEAR, END_YEAR)]\n",
    "\n",
    "        # --- Clean Members Data ---\n",
    "        member_cols_to_keep = [\n",
    "            MEMBER_ID_COL, MEMBER_DOB_COL, MEMBER_JOIN_DATE_COL,\n",
    "            MEMBER_GENDER_COL, MEMBER_POSTCODE_COL, MEMBER_SCHEME_COL, MEMBER_STATUS_COL\n",
    "        ]\n",
    "        df_members = df_members[member_cols_to_keep]\n",
    "        df_members[MEMBER_JOIN_DATE_COL] = pd.to_datetime(df_members[MEMBER_JOIN_DATE_COL], errors='coerce')\n",
    "        df_members[MEMBER_ID_COL] = df_members[MEMBER_ID_COL].astype(str)\n",
    "        df_claims[CLAIM_MEMBER_ID_COL] = df_claims[CLAIM_MEMBER_ID_COL].astype(str)\n",
    "        df_members = df_members.drop_duplicates(subset=[MEMBER_ID_COL])\n",
    "        \n",
    "        print(f\"Data cleaning complete. Claims: {len(df_claims)}, Members: {len(df_members)}\")\n",
    "        return df_claims, df_members\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: One of the files was not found.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data loading: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. CREATE THE \"SCAFFOLD\" (MEMBER-YEAR) DATASET ---\n",
    "# ==============================================================================\n",
    "\n",
    "def create_scaffold(df_claims, df_members, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Creates the master (member, year) table with aggregated claims\n",
    "    and flags for specific conditions.\n",
    "    \"\"\"\n",
    "    print(\"Creating member-year scaffold...\")\n",
    "    \n",
    "    # --- A. Aggregate all claim transactions by (member, year) ---\n",
    "    def had_condition(series, keyword):\n",
    "        return (series.str.contains(keyword, case=False, na=False)).any()\n",
    "        \n",
    "    aggregation_rules = {\n",
    "        'total_claim_amount': (CLAIM_AMOUNT_COL, 'sum'),\n",
    "        'total_claim_count': (CLAIM_AMOUNT_COL, 'count'),\n",
    "        'total_los': (CLAIM_LOS_COL, 'sum'),\n",
    "        'count_inpatient': (CLAIM_TYPE_COL, lambda x: (x == 'Inpatient').any()),\n",
    "        'unique_conditions_count': (CONDITION_CATEGORY_COL, 'nunique'),\n",
    "        'had_cardio': (CONDITION_CATEGORY_COL, lambda s: had_condition(s, 'Cardio')),\n",
    "        'had_musculo': (CONDITION_CATEGORY_COL, lambda s: had_condition(s, 'Musculo')),\n",
    "        'had_gastro': (CONDITION_CATEGORY_COL, lambda s: had_condition(s, 'Gastro')),\n",
    "        'had_chemo': (CONDITION_CATEGORY_COL, lambda s: had_condition(s, 'Chemo')),\n",
    "    }\n",
    "\n",
    "    df_agg = df_claims.groupby([CLAIM_MEMBER_ID_COL, 'claim_year']).agg(**aggregation_rules).reset_index()\n",
    "    df_agg = df_agg.rename(columns={'claim_year': 'year', CLAIM_MEMBER_ID_COL: MEMBER_ID_COL})\n",
    "\n",
    "    # --- B. Create the master (member, year) scaffold ---\n",
    "    all_members = df_members[MEMBER_ID_COL].unique()\n",
    "    all_years = list(range(start_year, end_year + 1))\n",
    "    \n",
    "    scaffold_index = pd.MultiIndex.from_product([all_members, all_years], names=[MEMBER_ID_COL, 'year'])\n",
    "    df_scaffold = pd.DataFrame(index=scaffold_index).reset_index()\n",
    "\n",
    "    # --- C. Join scaffold with aggregated claims ---\n",
    "    df_master = pd.merge(df_scaffold, df_agg, on=[MEMBER_ID_COL, 'year'], how='left')\n",
    "\n",
    "    # --- D. Fill ZEROS for all claim-related columns ---\n",
    "    cols_to_fill = list(aggregation_rules.keys())\n",
    "    df_master[cols_to_fill] = df_master[cols_to_fill].fillna(0)\n",
    "\n",
    "    # --- E. Join static membership data ---\n",
    "    df_master = pd.merge(df_master, df_members, on=MEMBER_ID_COL, how='left')\n",
    "    df_master = df_master.dropna(subset=[MEMBER_DOB_COL, MEMBER_JOIN_DATE_COL])\n",
    "    \n",
    "    print(\"Scaffold creation complete.\")\n",
    "    return df_master\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. FEATURE ENGINEERING (CORRECTED) ---\n",
    "# ==============================================================================\n",
    "\n",
    "def engineer_features(df_master):\n",
    "    \"\"\"\n",
    "    Creates time-based features (lags, rolling averages) and the target.\n",
    "    \"\"\"\n",
    "    print(\"Engineering features...\")\n",
    "    df_master = df_master.sort_values(by=[MEMBER_ID_COL, 'year']).reset_index(drop=True)\n",
    "    \n",
    "    # Create a helper column first\n",
    "    df_master['is_claim_free_year'] = (df_master['total_claim_count'] == 0).astype(int)\n",
    "    \n",
    "    g = df_master.groupby(MEMBER_ID_COL)\n",
    "    \n",
    "    # --- A. Dynamic Demographic Features ---\n",
    "    df_master['age'] = df_master['year'] - df_master[MEMBER_DOB_COL]\n",
    "    df_master['tenure'] = df_master['year'] - df_master[MEMBER_JOIN_DATE_COL].dt.year\n",
    "    df_master['tenure'] = df_master['tenure'].clip(lower=0)\n",
    "    \n",
    "    # --- B. Lagged Features ---\n",
    "    # .shift() on a groupby is a transform and preserves the original index, so it's safe.\n",
    "    df_master['cost_lag1'] = g['total_claim_amount'].shift(1)\n",
    "    df_master['cost_lag2'] = g['total_claim_amount'].shift(2)\n",
    "    df_master['claim_count_lag1'] = g['total_claim_count'].shift(1)\n",
    "    df_master['inpatient_count_lag1'] = g['count_inpatient'].shift(1)\n",
    "    \n",
    "    condition_lags = ['unique_conditions_count', 'had_cardio', 'had_musculo', 'had_gastro', 'had_chemo']\n",
    "    for col in condition_lags:\n",
    "        df_master[f'{col}_lag1'] = g[col].shift(1)\n",
    "\n",
    "    # --- C. Rolling Window Features (THIS IS THE FIX) ---\n",
    "    # .rolling() creates a MultiIndex. We must kill it with .reset_index()\n",
    "    \n",
    "    df_master['cost_avg_3yr'] = g['total_claim_amount'].rolling(3, min_periods=1).mean().reset_index(level=0, drop=True).shift(1)\n",
    "    df_master['cost_max_3yr'] = g['total_claim_amount'].rolling(3, min_periods=1).max().reset_index(level=0, drop=True).shift(1)\n",
    "    df_master['cost_std_3yr'] = g['total_claim_amount'].rolling(3, min_periods=1).std().reset_index(level=0, drop=True).shift(1)\n",
    "    \n",
    "    df_master['claim_free_years_3yr'] = g['is_claim_free_year'].rolling(3, min_periods=1).sum().reset_index(level=0, drop=True).shift(1)\n",
    "\n",
    "    df_master['cost_avg_5yr'] = g['total_claim_amount'].rolling(5, min_periods=1).mean().reset_index(level=0, drop=True).shift(1)\n",
    "    df_master['cost_max_5yr'] = g['total_claim_amount'].rolling(5, min_periods=1).max().reset_index(level=0, drop=True).shift(1)\n",
    "    \n",
    "    # --- D. Create Target Variable (y) ---\n",
    "    df_master['target_cost_next_year'] = g['total_claim_amount'].shift(-1)\n",
    "    \n",
    "    # --- E. Final Cleanup ---\n",
    "    df_model_data = df_master.dropna(subset=['target_cost_next_year'])\n",
    "    df_model_data = df_model_data.dropna(subset=['cost_lag1', 'cost_avg_3yr'])\n",
    "    df_model_data = df_model_data.fillna(0) # Fill remaining NaNs (like std dev)\n",
    "\n",
    "    # Drop the helper column\n",
    "    df_model_data = df_model_data.drop(columns=['is_claim_free_year'], errors='ignore')\n",
    "    df_master = df_master.drop(columns=['is_claim_free_year'], errors='ignore')\n",
    "\n",
    "    print(f\"Feature engineering complete. Model-ready data has {len(df_model_data)} rows.\")\n",
    "    return df_model_data, df_master \n",
    "\n",
    "# ==============================================================================\n",
    "# --- 5. MODEL TRAINING & VALIDATION (TEST ON 2025) ---\n",
    "# ==============================================================================\n",
    "\n",
    "def train_and_validate(df_model_data, validation_year):\n",
    "    \"\"\"\n",
    "    Trains the model and validates it on the holdout year (2025).\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Phase 1: Training & Validation on {validation_year} ---\")\n",
    "    \n",
    "    # --- A. Define Features (X) and Target (y) ---\n",
    "    target = 'target_cost_next_year'\n",
    "    \n",
    "    static_features = [\n",
    "        MEMBER_GENDER_COL, MEMBER_POSTCODE_COL, \n",
    "        MEMBER_SCHEME_COL, MEMBER_STATUS_COL\n",
    "    ]\n",
    "    \n",
    "    numeric_features = [\n",
    "        'age', 'tenure', \n",
    "        'cost_lag1', 'cost_lag2', 'claim_count_lag1', 'inpatient_count_lag1',\n",
    "        'cost_avg_3yr', 'cost_max_3yr', 'cost_std_3yr', 'claim_free_years_3yr',\n",
    "        'cost_avg_5yr', 'cost_max_5yr',\n",
    "        'unique_conditions_count_lag1', 'had_cardio_lag1', 'had_musculo_lag1',\n",
    "        'had_gastro_lag1', 'had_chemo_lag1'\n",
    "    ]\n",
    "    \n",
    "    features = numeric_features + static_features\n",
    "    features = [f for f in features if f in df_model_data.columns]\n",
    "    static_features = [f for f in static_features if f in df_model_data.columns]\n",
    "    \n",
    "    print(f\"\\nTraining with {len(features)} features.\")\n",
    "    \n",
    "    # --- B. Encode Categorical Features ---\n",
    "    encoders = {}\n",
    "    for col in static_features:\n",
    "        le = LabelEncoder()\n",
    "        df_model_data[col] = le.fit_transform(df_model_data[col].astype(str))\n",
    "        encoders[col] = le\n",
    "    \n",
    "    # --- C. Create Train and Test (Out-of-Time) Split ---\n",
    "    X_test_df = df_model_data[df_model_data['year'] == validation_year - 1].copy()\n",
    "    X_train_df = df_model_data[df_model_data['year'] < validation_year - 1].copy()\n",
    "\n",
    "    X_train = X_train_df[features]\n",
    "    y_train = X_train_df[target]\n",
    "    \n",
    "    X_test = X_test_df[features]\n",
    "    y_test = X_test_df[target]\n",
    "    \n",
    "    print(f\"Training on {len(X_train)} rows (Years {X_train_df['year'].min()}-{X_train_df['year'].max()})\")\n",
    "    print(f\"Validating on {len(X_test)} rows (Predicting year {validation_year})\")\n",
    "\n",
    "    # --- D. Train the LightGBM Tweedie Regressor ---\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        objective='tweedie',\n",
    "        tweedie_variance_power=1.5,\n",
    "        metric='rmse',\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.01,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    lgb_model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_test, y_test)],\n",
    "                  eval_metric='rmse',\n",
    "                  callbacks=[lgb.early_stopping(100, verbose=True)],\n",
    "                  categorical_feature=static_features\n",
    "                 )\n",
    "    \n",
    "    # --- E. Evaluate Model Performance on 2025 Data ---\n",
    "    y_pred_2025 = lgb_model.predict(X_test)\n",
    "    y_pred_2025[y_pred_2025 < 0] = 0 \n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred_2025)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_2025))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"--- VALIDATION RESULTS (PREDICTING 2025) ---\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"Actual Avg Cost 2025: {y_test.mean():.2f}\")\n",
    "    print(f\"Predicted Avg Cost 2025: {y_pred_2025.mean():.2f}\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    # --- F. Show Feature Importance ---\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': lgb_model.feature_name_,\n",
    "        'importance': lgb_model.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    print(\"\\n--- Top 20 Most Important Features (from validation) ---\")\n",
    "    print(importance_df.head(20))\n",
    "    \n",
    "    # --- G. Save validation predictions (with index fix) ---\n",
    "    df_val_output = X_test_df.reset_index(drop=True)\n",
    "    df_val_output['actual_cost_2025'] = y_test.reset_index(drop=True)\n",
    "    df_val_output['predicted_cost_2025'] = y_pred_2025 \n",
    "\n",
    "    output_filename = 'validation_predictions_for_2025_method1.csv'\n",
    "    df_val_output.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nSaved detailed 2025 validation predictions to: {output_filename}\")\n",
    "    \n",
    "    return lgb_model, encoders, features, static_features\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 6. MAIN EXECUTION (TRAINING) ---\n",
    "# ==============================================================================\n",
    "\n",
    "def main_train():\n",
    "    print(\"--- Starting Model Training (Part 1) ---\")\n",
    "    \n",
    "    df_claims, df_members = load_data(CLAIM_FILE_PATH, MEMBER_FILE_PATH)\n",
    "    if df_claims is None or df_members is None:\n",
    "        print(\"Failed to load data. Exiting.\")\n",
    "        return\n",
    "\n",
    "    df_master = create_scaffold(df_claims, df_members, START_YEAR, END_YEAR)\n",
    "    df_model_data, _ = engineer_features(df_master.copy())\n",
    "    \n",
    "    if df_model_data.empty:\n",
    "        print(\"Error: No model data could be created.\")\n",
    "        return\n",
    "\n",
    "    val_model, encoders, features, static_features = train_and_validate(df_model_data, VALIDATION_YEAR)\n",
    "    \n",
    "    print(\"\\n--- Re-training final model on all data (2019-2025) ---\")\n",
    "    \n",
    "    target = 'target_cost_next_year'\n",
    "    X_full = df_model_data[features]\n",
    "    y_full = df_model_data[target]\n",
    "    \n",
    "    final_model = lgb.LGBMRegressor(**val_model.get_params())\n",
    "    final_model.set_params(n_estimators=val_model.best_iteration_)\n",
    "    \n",
    "    final_model.fit(X_full, y_full, categorical_feature=static_features)\n",
    "    \n",
    "    print(\"Final model re-trained successfully.\")\n",
    "    \n",
    "    MODEL_PATH = 'claim_model_method1.lgb'\n",
    "    ENCODERS_PATH = 'data_encoders.pkl'\n",
    "    FEATURES_PATH = 'feature_list.pkl'\n",
    "    \n",
    "    joblib.dump(final_model, MODEL_PATH)\n",
    "    joblib.dump(encoders, ENCODERS_PATH)\n",
    "    joblib.dump(features, FEATURES_PATH)\n",
    "    \n",
    "    print(f\"\\nArtifacts saved:\")\n",
    "    print(f\"Model: {MODEL_PATH}\")\n",
    "    print(f\"Encoders: {ENCODERS_PATH}\")\n",
    "    print(f\"Feature List: {FEATURES_PATH}\")\n",
    "    \n",
    "    print(\"\\n--- Training (Part 1) Complete ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94b8ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b004a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n",
      "--- Starting Model Training ---\n",
      "Successfully loaded mercer_project/claims_200k.csv and mercer_project/membership_120k.csv\n",
      "Data cleaning complete. Claims: 199845, Members: 120000\n",
      "Creating member-year scaffold...\n",
      "Scaffold creation complete.\n",
      "Engineering features...\n",
      "Feature engineering complete. Model-ready data has 720000 rows.\n",
      "\n",
      "--- Phase 1: Training & Validation on 2025 ---\n",
      "\n",
      "Training with 21 features: ['age', 'tenure', 'cost_lag1', 'cost_lag2', 'claim_count_lag1', 'inpatient_count_lag1', 'cost_avg_3yr', 'cost_max_3yr', 'cost_std_3yr', 'claim_free_years_3yr', 'cost_avg_5yr', 'cost_max_5yr', 'unique_conditions_count_lag1', 'had_cardio_lag1', 'had_musculo_lag1', 'had_gastro_lag1', 'had_chemo_lag1', 'Gender', 'Short Post Code of Member', 'Scheme Category/ Section Name', 'Status of Member']\n",
      "Training on 600000 rows (Years 2019-2023)\n",
      "Validating on 120000 rows (Predicting year 2025)\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 27629\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 20\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 9.024763\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[977]\tvalid_0's rmse: 107637\n",
      "\n",
      "==================================================\n",
      "--- VALIDATION RESULTS (PREDICTING 2025) ---\n",
      "R-squared: 0.0851\n",
      "RMSE: 107637.28\n",
      "Actual Avg Cost: 8478.65\n",
      "Predicted Avg Cost: 8223.25\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Top 20 Important Features ---\n",
      "                          feature  importance\n",
      "18      Short_Post_Code_of_Member       15180\n",
      "0                             age        1909\n",
      "10                   cost_avg_5yr        1892\n",
      "1                          tenure        1537\n",
      "9            claim_free_years_3yr        1236\n",
      "2                       cost_lag1        1187\n",
      "19  Scheme_Category/_Section_Name        1164\n",
      "6                    cost_avg_3yr         967\n",
      "3                       cost_lag2         911\n",
      "4                claim_count_lag1         798\n",
      "11                   cost_max_5yr         652\n",
      "8                    cost_std_3yr         603\n",
      "7                    cost_max_3yr         412\n",
      "17                         Gender         340\n",
      "20               Status_of_Member         183\n",
      "12   unique_conditions_count_lag1         165\n",
      "14               had_musculo_lag1          72\n",
      "5            inpatient_count_lag1          50\n",
      "15                had_gastro_lag1          36\n",
      "13                had_cardio_lag1          16\n",
      "Saved validation predictions to: validation_predictions_2025.csv\n",
      "\n",
      "--- Re-training final model on all data ---\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28297\n",
      "[LightGBM] [Info] Number of data points in the train set: 720000, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 9.028216\n",
      "\n",
      "Artifacts saved successfully.\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "CLAIM_FILE_PATH = 'mercer_project/claims_200k.csv'\n",
    "MEMBER_FILE_PATH = 'mercer_project/membership_120k.csv'\n",
    "\n",
    "# Claim Columns\n",
    "CLAIM_AMOUNT_COL = 'Claim Amount'\n",
    "CLAIM_DATE_COL = 'Incurred Date'\n",
    "CLAIM_MEMBER_ID_COL = 'Unique Member Reference'\n",
    "CLAIM_TYPE_COL = 'Claim Type'\n",
    "CLAIM_LOS_COL = 'Calculated Length of Service'\n",
    "CONDITION_CATEGORY_COL = 'Condition Category'\n",
    "\n",
    "# Member Columns\n",
    "MEMBER_ID_COL = 'Unique Member Reference'\n",
    "MEMBER_DOB_COL = 'Year of Birth'\n",
    "MEMBER_JOIN_DATE_COL = 'Original Date of Joining'\n",
    "MEMBER_GENDER_COL = 'Gender'\n",
    "MEMBER_POSTCODE_COL = 'Short Post Code of Member'\n",
    "MEMBER_SCHEME_COL = 'Scheme Category/ Section Name'\n",
    "MEMBER_STATUS_COL = 'Status of Member'\n",
    "\n",
    "START_YEAR = 2019\n",
    "END_YEAR = 2025\n",
    "VALIDATION_YEAR = 2025\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. LOAD AND CLEAN DATA\n",
    "# ==============================================================================\n",
    "def load_data(claim_path, member_path):\n",
    "    try:\n",
    "        try:\n",
    "            df_claims = pd.read_csv(claim_path)\n",
    "            df_members = pd.read_csv(member_path)\n",
    "        except UnicodeDecodeError:\n",
    "            df_claims = pd.read_csv(claim_path, encoding='latin1')\n",
    "            df_members = pd.read_csv(member_path, encoding='latin1')\n",
    "            \n",
    "        print(f\"Successfully loaded {claim_path} and {member_path}\")\n",
    "\n",
    "        claim_cols_to_keep = [\n",
    "            CLAIM_MEMBER_ID_COL, CLAIM_DATE_COL, CLAIM_AMOUNT_COL,\n",
    "            CLAIM_TYPE_COL, CLAIM_LOS_COL, CONDITION_CATEGORY_COL\n",
    "        ]\n",
    "        df_claims = df_claims[claim_cols_to_keep]\n",
    "        \n",
    "        df_claims[CLAIM_DATE_COL] = pd.to_datetime(df_claims[CLAIM_DATE_COL], errors='coerce')\n",
    "        df_claims['claim_year'] = df_claims[CLAIM_DATE_COL].dt.year\n",
    "        df_claims[CLAIM_AMOUNT_COL] = pd.to_numeric(df_claims[CLAIM_AMOUNT_COL], errors='coerce')\n",
    "        df_claims[CLAIM_LOS_COL] = pd.to_numeric(df_claims[CLAIM_LOS_COL], errors='coerce')\n",
    "        df_claims = df_claims.dropna(subset=[CLAIM_DATE_COL, CLAIM_MEMBER_ID_COL, CLAIM_AMOUNT_COL])\n",
    "        df_claims = df_claims[df_claims['claim_year'].between(START_YEAR, END_YEAR)]\n",
    "\n",
    "        member_cols_to_keep = [\n",
    "            MEMBER_ID_COL, MEMBER_DOB_COL, MEMBER_JOIN_DATE_COL,\n",
    "            MEMBER_GENDER_COL, MEMBER_POSTCODE_COL, MEMBER_SCHEME_COL, MEMBER_STATUS_COL\n",
    "        ]\n",
    "        df_members = df_members[member_cols_to_keep]\n",
    "        df_members[MEMBER_JOIN_DATE_COL] = pd.to_datetime(df_members[MEMBER_JOIN_DATE_COL], errors='coerce')\n",
    "        df_members[MEMBER_ID_COL] = df_members[MEMBER_ID_COL].astype(str)\n",
    "        df_claims[CLAIM_MEMBER_ID_COL] = df_claims[CLAIM_MEMBER_ID_COL].astype(str)\n",
    "        df_members = df_members.drop_duplicates(subset=[MEMBER_ID_COL])\n",
    "        \n",
    "        print(f\"Data cleaning complete. Claims: {len(df_claims)}, Members: {len(df_members)}\")\n",
    "        return df_claims, df_members\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: One of the files was not found.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data loading: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. CREATE MEMBER-YEAR SCAFFOLD\n",
    "# ==============================================================================\n",
    "def create_scaffold(df_claims, df_members, start_year, end_year):\n",
    "    print(\"Creating member-year scaffold...\")\n",
    "\n",
    "    def had_condition(series, keyword):\n",
    "        return (series.str.contains(keyword, case=False, na=False)).any()\n",
    "\n",
    "    aggregation_rules = {\n",
    "        'total_claim_amount': (CLAIM_AMOUNT_COL, 'sum'),\n",
    "        'total_claim_count': (CLAIM_AMOUNT_COL, 'count'),\n",
    "        'total_los': (CLAIM_LOS_COL, 'sum'),\n",
    "        'count_inpatient': (CLAIM_TYPE_COL, lambda x: (x == 'Inpatient').any()),\n",
    "        'unique_conditions_count': (CONDITION_CATEGORY_COL, 'nunique'),\n",
    "        'had_cardio': (CONDITION_CATEGORY_COL, lambda s: had_condition(s, 'Cardio')),\n",
    "        'had_musculo': (CONDITION_CATEGORY_COL, lambda s: had_condition(s, 'Musculo')),\n",
    "        'had_gastro': (CONDITION_CATEGORY_COL, lambda s: had_condition(s, 'Gastro')),\n",
    "        'had_chemo': (CONDITION_CATEGORY_COL, lambda s: had_condition(s, 'Chemo')),\n",
    "    }\n",
    "\n",
    "    # perform aggregation\n",
    "    df_agg = df_claims.groupby([CLAIM_MEMBER_ID_COL, 'claim_year']).agg(**aggregation_rules).reset_index()\n",
    "    df_agg = df_agg.rename(columns={'claim_year': 'year', CLAIM_MEMBER_ID_COL: MEMBER_ID_COL})\n",
    "\n",
    "    # scaffold\n",
    "    all_members = df_members[MEMBER_ID_COL].unique()\n",
    "    all_years = list(range(start_year, end_year + 1))\n",
    "    scaffold_index = pd.MultiIndex.from_product([all_members, all_years], names=[MEMBER_ID_COL, 'year'])\n",
    "    df_scaffold = pd.DataFrame(index=scaffold_index).reset_index()\n",
    "\n",
    "    df_master = pd.merge(df_scaffold, df_agg, on=[MEMBER_ID_COL, 'year'], how='left')\n",
    "\n",
    "    # fill zeros for numeric-like columns\n",
    "    cols_to_fill = list(aggregation_rules.keys())\n",
    "    df_master[cols_to_fill] = df_master[cols_to_fill].fillna(0)\n",
    "\n",
    "    # Ensure boolean-like aggregated cols are numeric 0/1 (int)\n",
    "    bool_like_cols = ['count_inpatient', 'had_cardio', 'had_musculo', 'had_gastro', 'had_chemo']\n",
    "    for c in bool_like_cols:\n",
    "        if c in df_master.columns:\n",
    "            # convert True/False to 1/0, and ensure numeric\n",
    "            df_master[c] = df_master[c].replace({True: 1, False: 0}).fillna(0).astype(int)\n",
    "\n",
    "    # merge membership static fields\n",
    "    df_master = pd.merge(df_master, df_members, on=MEMBER_ID_COL, how='left')\n",
    "    df_master = df_master.dropna(subset=[MEMBER_DOB_COL, MEMBER_JOIN_DATE_COL])\n",
    "\n",
    "    print(\"Scaffold creation complete.\")\n",
    "    return df_master\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. FEATURE ENGINEERING (FIXED ROLLING + SAFE CASTS)\n",
    "# ==============================================================================\n",
    "def engineer_features(df_master):\n",
    "    print(\"Engineering features...\")\n",
    "    df_master = df_master.sort_values(by=[MEMBER_ID_COL, 'year']).reset_index(drop=True)\n",
    "\n",
    "    g = df_master.groupby(MEMBER_ID_COL)\n",
    "\n",
    "    def safe_rolling(series, window, func):\n",
    "        # Use groupby on member id, rolling, aggregate, shift, reset index to align\n",
    "        return (\n",
    "            series.groupby(df_master[MEMBER_ID_COL])\n",
    "            .rolling(window, min_periods=1)\n",
    "            .agg(func)\n",
    "            .shift(1)\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    # helper\n",
    "    df_master['is_claim_free_year'] = (df_master['total_claim_count'] == 0).astype(int)\n",
    "\n",
    "    # demographics\n",
    "    df_master['age'] = df_master['year'] - pd.to_numeric(df_master[MEMBER_DOB_COL], errors='coerce')\n",
    "    df_master['tenure'] = df_master['year'] - df_master[MEMBER_JOIN_DATE_COL].dt.year\n",
    "    df_master['tenure'] = df_master['tenure'].clip(lower=0)\n",
    "\n",
    "    # Lag features (ensure numeric and fill NaNs)\n",
    "    df_master['cost_lag1'] = g['total_claim_amount'].shift(1).fillna(0).astype(float)\n",
    "    df_master['cost_lag2'] = g['total_claim_amount'].shift(2).fillna(0).astype(float)\n",
    "    df_master['claim_count_lag1'] = g['total_claim_count'].shift(1).fillna(0).astype(float)\n",
    "\n",
    "    # inpatient/count and condition lags: ensure they become numeric 0/1 or numeric\n",
    "    df_master['inpatient_count_lag1'] = g['count_inpatient'].shift(1).fillna(0).astype(int)\n",
    "\n",
    "    condition_lags = ['unique_conditions_count', 'had_cardio', 'had_musculo', 'had_gastro', 'had_chemo']\n",
    "    for col in condition_lags:\n",
    "        if col in df_master.columns:\n",
    "            # unique_conditions_count makes sense as numeric; had_* are 0/1\n",
    "            if col == 'unique_conditions_count':\n",
    "                df_master[f'{col}_lag1'] = g[col].shift(1).fillna(0).astype(float)\n",
    "            else:\n",
    "                df_master[f'{col}_lag1'] = g[col].shift(1).fillna(0).astype(int)\n",
    "\n",
    "    # Rolling 3-year features (safe)\n",
    "    df_master['cost_avg_3yr'] = safe_rolling(df_master['total_claim_amount'], 3, 'mean').fillna(0).astype(float)\n",
    "    df_master['cost_max_3yr'] = safe_rolling(df_master['total_claim_amount'], 3, 'max').fillna(0).astype(float)\n",
    "    # std can be NaN if single period; fill with 0\n",
    "    df_master['cost_std_3yr'] = safe_rolling(df_master['total_claim_amount'], 3, 'std').fillna(0).astype(float)\n",
    "    df_master['claim_free_years_3yr'] = safe_rolling(df_master['is_claim_free_year'], 3, 'sum').fillna(0).astype(int)\n",
    "\n",
    "    # Rolling 5-year features\n",
    "    df_master['cost_avg_5yr'] = safe_rolling(df_master['total_claim_amount'], 5, 'mean').fillna(0).astype(float)\n",
    "    df_master['cost_max_5yr'] = safe_rolling(df_master['total_claim_amount'], 5, 'max').fillna(0).astype(float)\n",
    "\n",
    "    # Target\n",
    "    df_master['target_cost_next_year'] = g['total_claim_amount'].shift(-1)\n",
    "\n",
    "    # Prepare model dataset\n",
    "    df_model_data = df_master.dropna(subset=['target_cost_next_year']).copy()\n",
    "\n",
    "    # Make sure required lag features exist and are numeric; drop rows where critical lags are missing\n",
    "    required_for_model = ['cost_lag1', 'cost_avg_3yr']\n",
    "    missing_req = [c for c in required_for_model if c not in df_model_data.columns]\n",
    "    if missing_req:\n",
    "        raise ValueError(f\"Missing required features after engineering: {missing_req}\")\n",
    "\n",
    "    df_model_data = df_model_data.dropna(subset=required_for_model)\n",
    "\n",
    "    # Final fill for any remaining numeric NaNs\n",
    "    numeric_cols = df_model_data.select_dtypes(include=['number']).columns.tolist()\n",
    "    df_model_data[numeric_cols] = df_model_data[numeric_cols].fillna(0)\n",
    "\n",
    "    # Ensure boolean-like columns are numeric\n",
    "    bool_like_lag_cols = ['inpatient_count_lag1', 'had_cardio_lag1', 'had_musculo_lag1', 'had_gastro_lag1', 'had_chemo_lag1']\n",
    "    for c in bool_like_lag_cols:\n",
    "        if c in df_model_data.columns:\n",
    "            df_model_data[c] = pd.to_numeric(df_model_data[c], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    # Drop helper column from final\n",
    "    df_model_data = df_model_data.drop(columns=['is_claim_free_year'], errors='ignore')\n",
    "    df_master = df_master.drop(columns=['is_claim_free_year'], errors='ignore')\n",
    "\n",
    "    print(f\"Feature engineering complete. Model-ready data has {len(df_model_data)} rows.\")\n",
    "    return df_model_data, df_master\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 5. TRAIN & VALIDATE MODEL\n",
    "# ==============================================================================\n",
    "def train_and_validate(df_model_data, validation_year):\n",
    "    print(f\"\\n--- Phase 1: Training & Validation on {validation_year} ---\")\n",
    "    \n",
    "    target = 'target_cost_next_year'\n",
    "\n",
    "    static_features = [\n",
    "        MEMBER_GENDER_COL, MEMBER_POSTCODE_COL, \n",
    "        MEMBER_SCHEME_COL, MEMBER_STATUS_COL\n",
    "    ]\n",
    "\n",
    "    numeric_features = [\n",
    "        'age', 'tenure', \n",
    "        'cost_lag1', 'cost_lag2', 'claim_count_lag1', 'inpatient_count_lag1',\n",
    "        'cost_avg_3yr', 'cost_max_3yr', 'cost_std_3yr', 'claim_free_years_3yr',\n",
    "        'cost_avg_5yr', 'cost_max_5yr',\n",
    "        'unique_conditions_count_lag1', 'had_cardio_lag1', 'had_musculo_lag1',\n",
    "        'had_gastro_lag1', 'had_chemo_lag1'\n",
    "    ]\n",
    "\n",
    "    features = numeric_features + static_features\n",
    "    features = [f for f in features if f in df_model_data.columns]\n",
    "    static_features = [f for f in static_features if f in df_model_data.columns]\n",
    "\n",
    "    print(f\"\\nTraining with {len(features)} features: {features}\")\n",
    "    \n",
    "    # Encode categorical static features (in-place)\n",
    "    encoders = {}\n",
    "    for col in static_features:\n",
    "        le = LabelEncoder()\n",
    "        # fillna with a placeholder string before encoding\n",
    "        df_model_data[col] = df_model_data[col].fillna('MISSING').astype(str)\n",
    "        df_model_data[col] = le.fit_transform(df_model_data[col])\n",
    "        encoders[col] = le\n",
    "\n",
    "    # Ensure all feature columns are numeric\n",
    "    for f in features:\n",
    "        if f in df_model_data.columns:\n",
    "            if df_model_data[f].dtype == 'object':\n",
    "                # try converting to numeric, else raise\n",
    "                df_model_data[f] = pd.to_numeric(df_model_data[f], errors='coerce')\n",
    "    # final numeric fill\n",
    "    df_model_data[features] = df_model_data[features].fillna(0)\n",
    "\n",
    "    # Split: note predicting validation_year, so training uses years < validation_year-1, test uses year == validation_year-1\n",
    "    X_test_df = df_model_data[df_model_data['year'] == validation_year - 1].copy()\n",
    "    X_train_df = df_model_data[df_model_data['year'] < validation_year - 1].copy()\n",
    "\n",
    "    X_train = X_train_df[features]\n",
    "    y_train = X_train_df[target]\n",
    "    X_test = X_test_df[features]\n",
    "    y_test = X_test_df[target]\n",
    "    \n",
    "    print(f\"Training on {len(X_train)} rows (Years {X_train_df['year'].min()}-{X_train_df['year'].max()})\")\n",
    "    print(f\"Validating on {len(X_test)} rows (Predicting year {validation_year})\")\n",
    "\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        objective='tweedie',\n",
    "        tweedie_variance_power=1.5,\n",
    "        metric='rmse',\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.01,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train with early stopping\n",
    "    lgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[lgb.early_stopping(100, verbose=True)],\n",
    "        categorical_feature=static_features\n",
    "    )\n",
    "\n",
    "    y_pred_2025 = lgb_model.predict(X_test)\n",
    "    y_pred_2025[y_pred_2025 < 0] = 0 \n",
    "\n",
    "    r2 = r2_score(y_test, y_pred_2025)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_2025))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"--- VALIDATION RESULTS (PREDICTING {validation_year}) ---\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"Actual Avg Cost: {y_test.mean():.2f}\")\n",
    "    print(f\"Predicted Avg Cost: {y_pred_2025.mean():.2f}\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': lgb_model.feature_name_,\n",
    "        'importance': lgb_model.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    print(\"\\n--- Top 20 Important Features ---\")\n",
    "    print(importance_df.head(20))\n",
    "\n",
    "    df_val_output = X_test_df.reset_index(drop=True)\n",
    "    df_val_output['actual_cost'] = y_test.reset_index(drop=True)\n",
    "    df_val_output['predicted_cost'] = y_pred_2025\n",
    "    df_val_output.to_csv('validation_predictions_2025.csv', index=False)\n",
    "    print(\"Saved validation predictions to: validation_predictions_2025.csv\")\n",
    "\n",
    "    return lgb_model, encoders, features, static_features\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 6. MAIN EXECUTION\n",
    "# ==============================================================================\n",
    "def main_train():\n",
    "    print(\"--- Starting Model Training ---\")\n",
    "    \n",
    "    df_claims, df_members = load_data(CLAIM_FILE_PATH, MEMBER_FILE_PATH)\n",
    "    if df_claims is None or df_members is None:\n",
    "        return\n",
    "\n",
    "    df_master = create_scaffold(df_claims, df_members, START_YEAR, END_YEAR)\n",
    "    df_model_data, _ = engineer_features(df_master.copy())\n",
    "    if df_model_data.empty:\n",
    "        print(\"Error: No model data created.\")\n",
    "        return\n",
    "\n",
    "    val_model, encoders, features, static_features = train_and_validate(df_model_data, VALIDATION_YEAR)\n",
    "\n",
    "    print(\"\\n--- Re-training final model on all data ---\")\n",
    "    target = 'target_cost_next_year'\n",
    "    X_full = df_model_data[features]\n",
    "    y_full = df_model_data[target]\n",
    "\n",
    "    final_model = lgb.LGBMRegressor(**val_model.get_params())\n",
    "    final_model.set_params(n_estimators=val_model.best_iteration_)\n",
    "    final_model.fit(X_full, y_full, categorical_feature=static_features)\n",
    "\n",
    "    joblib.dump(final_model, 'claim_model_method1.lgb')\n",
    "    joblib.dump(encoders, 'data_encoders.pkl')\n",
    "    joblib.dump(features, 'feature_list.pkl')\n",
    "\n",
    "    print(\"\\nArtifacts saved successfully.\")\n",
    "    print(\"--- Training Complete ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bf09d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Predicting Claim Costs for 2026 ---\n",
      "Building features up to 2025...\n",
      " 2026 Predictions saved to: predicted_claim_costs_2026.csv\n",
      "  Unique Member Reference  predicted_cost_2026\n",
      "0             MBR-0000001           720.837648\n",
      "1             MBR-0000002         12099.199959\n",
      "2             MBR-0000003         19843.582399\n",
      "3             MBR-0000004           905.112818\n",
      "4             MBR-0000005         11768.929380\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "MODEL_PATH = \"claim_model_method1.lgb\"\n",
    "ENCODER_PATH = \"data_encoders.pkl\"\n",
    "FEATURE_LIST_PATH = \"feature_list.pkl\"\n",
    "\n",
    "CLAIM_FILE_PATH = \"mercer_project/claims_200k.csv\"      # all historical claims up to 2025\n",
    "MEMBER_FILE_PATH = \"mercer_project/membership_120k.csv\" # full membership table\n",
    "OUTPUT_FILE = \"predicted_claim_costs_2026.csv\"\n",
    "\n",
    "START_YEAR = 2019\n",
    "PREDICT_YEAR = 2026\n",
    "\n",
    "# --- Column Names ---\n",
    "CLAIM_AMOUNT_COL = \"Claim Amount\"\n",
    "CLAIM_DATE_COL = \"Incurred Date\"\n",
    "CLAIM_MEMBER_ID_COL = \"Unique Member Reference\"\n",
    "CLAIM_TYPE_COL = \"Claim Type\"\n",
    "CLAIM_LOS_COL = \"Calculated Length of Service\"\n",
    "CONDITION_CATEGORY_COL = \"Condition Category\"\n",
    "\n",
    "MEMBER_ID_COL = \"Unique Member Reference\"\n",
    "MEMBER_DOB_COL = \"Year of Birth\"\n",
    "MEMBER_JOIN_DATE_COL = \"Original Date of Joining\"\n",
    "MEMBER_GENDER_COL = \"Gender\"\n",
    "MEMBER_POSTCODE_COL = \"Short Post Code of Member\"\n",
    "MEMBER_SCHEME_COL = \"Scheme Category/ Section Name\"\n",
    "MEMBER_STATUS_COL = \"Status of Member\"\n",
    "\n",
    "# ==============================================================================\n",
    "# --- FEATURE BUILDING\n",
    "# ==============================================================================\n",
    "\n",
    "def preprocess_for_prediction(df_claims, df_members, start_year, end_year):\n",
    "    \"\"\"Rebuilds features exactly like training, up to given end_year.\"\"\"\n",
    "    print(f\"Building features up to {end_year}...\")\n",
    "\n",
    "    df_claims[CLAIM_DATE_COL] = pd.to_datetime(df_claims[CLAIM_DATE_COL], errors=\"coerce\")\n",
    "    df_claims[\"claim_year\"] = df_claims[CLAIM_DATE_COL].dt.year\n",
    "    df_claims[CLAIM_AMOUNT_COL] = pd.to_numeric(df_claims[CLAIM_AMOUNT_COL], errors=\"coerce\")\n",
    "    df_claims[CLAIM_LOS_COL] = pd.to_numeric(df_claims[CLAIM_LOS_COL], errors=\"coerce\")\n",
    "    df_claims = df_claims.dropna(subset=[CLAIM_MEMBER_ID_COL, CLAIM_AMOUNT_COL])\n",
    "    df_claims = df_claims[df_claims[\"claim_year\"].between(start_year, end_year)]\n",
    "\n",
    "    def had_condition(series, keyword):\n",
    "        return (series.str.contains(keyword, case=False, na=False)).any()\n",
    "\n",
    "    aggregation_rules = {\n",
    "        \"total_claim_amount\": (CLAIM_AMOUNT_COL, \"sum\"),\n",
    "        \"total_claim_count\": (CLAIM_AMOUNT_COL, \"count\"),\n",
    "        \"total_los\": (CLAIM_LOS_COL, \"sum\"),\n",
    "        \"count_inpatient\": (CLAIM_TYPE_COL, lambda x: (x == \"Inpatient\").any()),\n",
    "        \"unique_conditions_count\": (CONDITION_CATEGORY_COL, \"nunique\"),\n",
    "        \"had_cardio\": (CONDITION_CATEGORY_COL, lambda s: had_condition(s, \"Cardio\")),\n",
    "        \"had_musculo\": (CONDITION_CATEGORY_COL, lambda s: had_condition(s, \"Musculo\")),\n",
    "        \"had_gastro\": (CONDITION_CATEGORY_COL, lambda s: had_condition(s, \"Gastro\")),\n",
    "        \"had_chemo\": (CONDITION_CATEGORY_COL, lambda s: had_condition(s, \"Chemo\")),\n",
    "    }\n",
    "\n",
    "    df_agg = df_claims.groupby([CLAIM_MEMBER_ID_COL, \"claim_year\"]).agg(**aggregation_rules).reset_index()\n",
    "    df_agg = df_agg.rename(columns={\"claim_year\": \"year\", CLAIM_MEMBER_ID_COL: MEMBER_ID_COL})\n",
    "\n",
    "    all_members = df_members[MEMBER_ID_COL].unique()\n",
    "    all_years = list(range(start_year, end_year + 1))\n",
    "    scaffold = pd.MultiIndex.from_product([all_members, all_years], names=[MEMBER_ID_COL, \"year\"])\n",
    "    df_scaffold = pd.DataFrame(index=scaffold).reset_index()\n",
    "\n",
    "    df_master = pd.merge(df_scaffold, df_agg, on=[MEMBER_ID_COL, \"year\"], how=\"left\")\n",
    "    df_master[list(aggregation_rules.keys())] = df_master[list(aggregation_rules.keys())].fillna(0)\n",
    "\n",
    "    bool_cols = [\"count_inpatient\", \"had_cardio\", \"had_musculo\", \"had_gastro\", \"had_chemo\"]\n",
    "    for c in bool_cols:\n",
    "        df_master[c] = df_master[c].replace({True: 1, False: 0}).astype(int)\n",
    "\n",
    "    df_master = pd.merge(df_master, df_members, on=MEMBER_ID_COL, how=\"left\")\n",
    "    df_master = df_master.sort_values(by=[MEMBER_ID_COL, \"year\"]).reset_index(drop=True)\n",
    "\n",
    "    g = df_master.groupby(MEMBER_ID_COL)\n",
    "    df_master[\"age\"] = df_master[\"year\"] - pd.to_numeric(df_master[MEMBER_DOB_COL], errors=\"coerce\")\n",
    "    df_master[MEMBER_JOIN_DATE_COL] = pd.to_datetime(df_master[MEMBER_JOIN_DATE_COL], errors=\"coerce\")\n",
    "    df_master[\"tenure\"] = df_master[\"year\"] - df_master[MEMBER_JOIN_DATE_COL].dt.year\n",
    "    df_master[\"tenure\"] = df_master[\"tenure\"].clip(lower=0)\n",
    "\n",
    "    # --- Lag & Rolling features ---\n",
    "    df_master[\"cost_lag1\"] = g[\"total_claim_amount\"].shift(1)\n",
    "    df_master[\"cost_lag2\"] = g[\"total_claim_amount\"].shift(2)\n",
    "    df_master[\"claim_count_lag1\"] = g[\"total_claim_count\"].shift(1)\n",
    "    df_master[\"inpatient_count_lag1\"] = g[\"count_inpatient\"].shift(1)\n",
    "    for col in [\"unique_conditions_count\", \"had_cardio\", \"had_musculo\", \"had_gastro\", \"had_chemo\"]:\n",
    "        df_master[f\"{col}_lag1\"] = g[col].shift(1)\n",
    "\n",
    "    rolling = lambda x, w, f: x.groupby(df_master[MEMBER_ID_COL]).rolling(w, min_periods=1).agg(f).shift(1).reset_index(level=0, drop=True)\n",
    "    df_master[\"cost_avg_3yr\"] = rolling(df_master[\"total_claim_amount\"], 3, \"mean\")\n",
    "    df_master[\"cost_max_3yr\"] = rolling(df_master[\"total_claim_amount\"], 3, \"max\")\n",
    "    df_master[\"cost_std_3yr\"] = rolling(df_master[\"total_claim_amount\"], 3, \"std\")\n",
    "    df_master[\"claim_free_years_3yr\"] = rolling((df_master[\"total_claim_count\"] == 0).astype(int), 3, \"sum\")\n",
    "    df_master[\"cost_avg_5yr\"] = rolling(df_master[\"total_claim_amount\"], 5, \"mean\")\n",
    "    df_master[\"cost_max_5yr\"] = rolling(df_master[\"total_claim_amount\"], 5, \"max\")\n",
    "\n",
    "    df_master = df_master.fillna(0)\n",
    "    return df_master\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- INFERENCE\n",
    "# ==============================================================================\n",
    "\n",
    "def run_inference():\n",
    "    print(\"--- Predicting Claim Costs for 2026 ---\")\n",
    "\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    encoders = joblib.load(ENCODER_PATH)\n",
    "    features = joblib.load(FEATURE_LIST_PATH)\n",
    "\n",
    "    df_claims = pd.read_csv(CLAIM_FILE_PATH)\n",
    "    df_members = pd.read_csv(MEMBER_FILE_PATH)\n",
    "\n",
    "    # Build features up to 2025 (we predict next year: 2026)\n",
    "    df_master = preprocess_for_prediction(df_claims, df_members, START_YEAR, 2025)\n",
    "\n",
    "    df_latest = df_master[df_master[\"year\"] == 2025].copy()\n",
    "\n",
    "    # Encode categoricals\n",
    "    for col, le in encoders.items():\n",
    "        if col in df_latest:\n",
    "            df_latest[col] = df_latest[col].astype(str)\n",
    "            unseen_mask = ~df_latest[col].isin(le.classes_)\n",
    "            if unseen_mask.any():\n",
    "                df_latest.loc[unseen_mask, col] = \"MISSING\"\n",
    "                le.classes_ = np.append(le.classes_, \"MISSING\")\n",
    "            df_latest[col] = le.transform(df_latest[col])\n",
    "\n",
    "    for f in features:\n",
    "        if f not in df_latest.columns:\n",
    "            df_latest[f] = 0\n",
    "    df_latest = df_latest[features].fillna(0)\n",
    "\n",
    "    preds = model.predict(df_latest)\n",
    "    preds = np.maximum(preds, 0)\n",
    "\n",
    "    df_output = pd.DataFrame({\n",
    "        MEMBER_ID_COL: df_master[df_master[\"year\"] == 2025][MEMBER_ID_COL].values,\n",
    "        \"predicted_cost_2026\": preds\n",
    "    })\n",
    "\n",
    "    df_output.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\" 2026 Predictions saved to: {OUTPUT_FILE}\")\n",
    "    print(df_output.head())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b8771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bbcab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
